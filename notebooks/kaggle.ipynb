{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572bd676-3508-4307-b190-0d44de2d2054",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We're going to train a model to detect anomalous electrocardiogram (ECG) signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a7b23-ee79-4dfc-95b4-c3ad014cda22",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "[The PTB Diagnostic ECG Database](https://www.physionet.org/physiobank/database/ptbdb/)\n",
    "\n",
    "> This dataset has been used in exploring heartbeat classification using deep neural network architectures, and observing some of the capabilities of transfer learning on it. The signals correspond to electrocardiogram (ECG) shapes of heartbeats for the normal case and the cases affected by different arrhythmias and myocardial infarction. These signals are preprocessed and segmented, with each segment corresponding to a heartbeat.\n",
    "- Number of Samples: 14552\n",
    "- Number of Categories: 2\n",
    "- Sampling Frequency: 125Hz\n",
    "- Data Source: Physionet's PTB Diagnostic Database\n",
    "\n",
    "**Note:** _All the samples are cropped, downsampled and padded with zeroes if necessary to the fixed dimension of 188._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c04f0-6b3c-44be-b57a-e474b9712892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import random\n",
    "import torch\n",
    "import torcheval\n",
    "from torcheval.metrics import BinaryConfusionMatrix, BinaryF1Score, BinaryPrecision, BinaryRecall, BinaryAccuracy\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from ecg import DATA_DIR\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d3fe2-987e-4374-ac07-0fbb34e48a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The datasets have been downloaded, converted to .parquet, and moved to the `data/` directory\n",
    "train_file = \"mitbih_train.parquet\"\n",
    "test_file = \"mitbih_test.parquet\"\n",
    "\n",
    "train_df = pl.read_parquet(DATA_DIR / train_file)\n",
    "test_df = pl.read_parquet(DATA_DIR / test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf86af-86c3-46e0-80fb-16bb32854b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10ac0f-6aa9-411c-8150-5d6df0624feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff915b3-74b5-44e1-b726-133aeb6262b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_file = \"ptbdb_abnormal.parquet\"\n",
    "normal_file = \"ptbdb_normal.parquet\"\n",
    "\n",
    "abnormal_df = pl.read_parquet(DATA_DIR / abnormal_file)\n",
    "normal_df = pl.read_parquet(DATA_DIR / normal_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75c4dc-79b9-4fa9-90e2-363e98b7791c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad0536-9eab-4758-8f73-c4eebc319d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_df.shape, normal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404deb6-ac02-45ad-868f-996fb5bfbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_df = normal_df.with_columns(pl.Series(\"target\", [\"normal\"] * len(normal_df)))\n",
    "# abnormal_df = abnormal_df.with_columns(pl.Series(\"target\", [\"abnormal\"] * len(normal_df)))\n",
    "\n",
    "\n",
    "df = pl.concat(\n",
    "    [\n",
    "        normal_df.with_columns(pl.Series(\"class\", [\"normal\"] * normal_df.shape[0])),\n",
    "        abnormal_df.with_columns(pl.Series(\"class\", [\"abnormal\"] * abnormal_df.shape[0]))\n",
    "    ],\n",
    "    how=\"vertical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899b886-3445-41c9-bd3f-9391705d7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(df: pl.DataFrame, class_name: str, samples: int = 100, opacity: float = 0.05) -> alt.Chart:\n",
    "    data = (\n",
    "        df.filter(\n",
    "            pl.col(\"class\") == class_name\n",
    "        )\n",
    "        .drop([\"class\", \"target\"])\n",
    "        .sample(n=samples) # we don't seed so that we can see different samples\n",
    "        .transpose()\n",
    "    )\n",
    "    plot_df = (\n",
    "        pl.concat([\n",
    "            data\n",
    "            .select(col)\n",
    "            .rename({col: \"signal\"})\n",
    "            .with_columns(\n",
    "                pl.Series(\"case\", [i] * data.shape[0]),\n",
    "                pl.Series(\"measurement\", list(range(data.shape[0]))),\n",
    "                pl.Series(\"color\", [0] * data.shape[0]),\n",
    "            )\n",
    "            for i, col in enumerate(data.columns)\n",
    "        ], \n",
    "        how=\"vertical\")\n",
    "        .with_row_index()\n",
    "    )\n",
    "    \n",
    "    return alt.Chart(plot_df, title=class_name).mark_line().encode(\n",
    "        x=alt.X(\"measurement\", title=\"measurement\"),\n",
    "        y=alt.Y(\"signal\", title=\"signal\", scale=alt.Scale(domain=[0.0, 1.2])),\n",
    "        color=alt.Color(\"color:N\", title=None),\n",
    "        detail=\"case\",\n",
    "        opacity=alt.value(opacity),\n",
    "    ).properties(height=300, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970acb5-acd5-4fd9-b5cc-4d6fbc0de0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(df, \"normal\", 100, 0.1) | plot_samples(df, \"abnormal\", 100, 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66d530-a4cd-4df0-ac9a-5676a420b24a",
   "metadata": {},
   "source": [
    "We can see that signals for both the \"normal\" cases and \"abnormal\" are between [0.0, 1.0]  and all cases start at or near 1.0.  \n",
    "From the 75th measurement (0.6 seconds from the start) the signals can spike to near 1.0 for a couple time steps before dropping abck to nominal levels.\n",
    "Some cases drop to 0.0 from about the 100th measurement (0.8 seconds from the start), this just means the signal ended early and has been padded with zeros until the 188th measurement.\n",
    "\n",
    "The normal cases have a spike in the signal at about the 35th measurement (0.28 seconds from the start) spread across ~30 measurements (0.24 seconds).  Those that don't end early tend to have another spike at the end spread across ~30 measurements.\n",
    "\n",
    "The abnormal cases are more variable, especially around where the first spike occurs in the normal cases.  The nominal level for each case is also more variable.\n",
    "\n",
    "Let's plot the mean and standard deviation across each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3bd0c-8b0d-4702-b059-59ac68fc1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_mean(df: pl.DataFrame, class_name: str, window: int = 5) -> alt.Chart:\n",
    "    period = f\"{window}i\"\n",
    "    \n",
    "    data = df.filter(pl.col(\"class\") == class_name).drop(\"class\")\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    \n",
    "    rolling_mean_df = (\n",
    "        mean.drop(\"target\").transpose()\n",
    "        .with_row_index()\n",
    "        .rolling(\"index\", period=period)\n",
    "        .agg([\n",
    "            pl.col(pl.Float64).mean()\n",
    "        ])\n",
    "        .drop(\"index\")\n",
    "    )\n",
    "    \n",
    "    rolling_std_df = (\n",
    "        std.drop(\"target\").transpose()\n",
    "        .with_row_index()\n",
    "        .rolling(\"index\", period=period)\n",
    "        .agg([\n",
    "            pl.col(pl.Float64).mean()\n",
    "        ])\n",
    "        .drop(\"index\")\n",
    "    )\n",
    "    margin = rolling_std_df * 2\n",
    "\n",
    "    # signals are always positive so we clip the low\n",
    "    lower_bound = (rolling_mean_df - margin).with_columns(pl.col(pl.Float64).clip(0.0, 1.0))\n",
    "    upper_bound = (rolling_mean_df + margin).with_columns(pl.col(pl.Float64).clip(0.0, 1.0))\n",
    "\n",
    "    plot_df = pl.concat([\n",
    "        rolling_mean_df.rename({\"column_0\": \"mean\"}),\n",
    "        lower_bound.rename({\"column_0\": \"lower\"}),\n",
    "        upper_bound.rename({\"column_0\": \"upper\"})\n",
    "    ], how=\"horizontal\").with_row_index()\n",
    "    \n",
    "    line = alt.Chart(plot_df).mark_line().encode(\n",
    "        x=alt.X(\"index\", title=\"measurement\"),\n",
    "        y=alt.Y(\"mean\", title=\"mean signal\")\n",
    "    )\n",
    "    \n",
    "    band = alt.Chart(plot_df, title=class_name).mark_area().encode(\n",
    "        x=alt.X(\"index\", title=\"measurement\"),\n",
    "        y=alt.Y(\"lower\", scale=alt.Scale(domain=[-0.1, 1.1])),\n",
    "        y2=alt.Y2(\"upper\"),\n",
    "        opacity=alt.value(0.25),\n",
    "    ).properties(height=300, width=400)\n",
    "    \n",
    "    return band + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae11c8-2e36-4db5-be57-504cff637c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_mean(df, \"normal\", 5) | plot_rolling_mean(df, \"abnormal\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54350d8-f936-495a-861f-4abb257aacb4",
   "metadata": {},
   "source": [
    "Here we've calculated the mean and standard deviation for each class at each measurement then taken a rolling average over the given window to smooth out the curve.\n",
    "The doubled standard deviation is shown as the upper and lower bounds, clipped to the range [1.0, 0.0].\n",
    "\n",
    "In the normal cases, we see a distinct spike at the 30th measurement and another more spread out spike about the 110th measurement.  The nominal signal level is ~0.2.  The signals end at about the 120th measurement (nominal level starts to decrease).\n",
    "\n",
    "In the abnormal cases, we see the same spikes but with less prominence, meaning when the spike occurs is variable.  The second spike is notably shifted forward to about the 90th measurement.  Overall, the signal has a much higher spread and the nominal signal level is ~0.25-0.3.   The signals tend to end 20 measurements earlier than normal signals, at the 100th measurement (nominal level starts to decrease)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf1c6b-968f-46c2-a5ee-53b8e44b7f41",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f83694-d8d5-4bc8-9e97-cdfde99872ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(torch.utils.data.Dataset):   \n",
    "    \"\"\"Dataset to sample cases from the training dataset.\n",
    "\n",
    "    TODO: randomly sample variable lengths\n",
    "    \"\"\"\n",
    "    def __init__(self, X: pl.DataFrame, y: pl.Series) -> None:\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.X = X.to_torch().to(torch.float32)\n",
    "        self.y = y.to_torch().to(torch.float32)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def get_row(self, idx: int) -> pl.DataFrame:\n",
    "        return pl.concat([self.target, self.data], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84157d10-35dd-403c-835b-4cce091baf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int) -> None:\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.input_dim = input_dim # (188)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Building an linear encoder with Linear layer followed by Relu activation function\n",
    "        # 188 ==> 16\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(self.input_dim, 128)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('linear2', nn.Linear(128, 64)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('linear3', nn.Linear(64, 32)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('linear4', nn.Linear(32, 16)),\n",
    "        ]))\n",
    "\n",
    "        # Building an linear decoder with Linear layer followed by Relu activation function\n",
    "        # The Sigmoid activation function outputs the value between 0 and 1\n",
    "        # 16 ==> 188\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(16, 32)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('linear2', nn.Linear(32, 64)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('linear3', nn.Linear(64, 128)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('linear4', nn.Linear(128, self.input_dim)),\n",
    "            (\"activation\", torch.nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x) \n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "        \n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, kernel_size: int, stride: int) -> None:\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.input_dim = input_dim # (188)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "\n",
    "        # Building an linear encoder with Linear layer followed by Relu activation function\n",
    "        # 188 ==> 16\n",
    "\n",
    "        assert (self.kernel_size % 2 != 0) # and (stride == 1)\n",
    "        pool_padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            # Conv1d Layer 1: Input Channels = 1, Output Channels = hidden_features\n",
    "            ('conv1', nn.Conv1d(1, self.hidden_dim, kernel_size=kernel_size, stride=self.stride)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('norm1', nn.BatchNorm1d(self.hidden_dim)),\n",
    "            ('pool1', nn.MaxPool1d(kernel_size=kernel_size, stride=1, padding=pool_padding)),\n",
    "\n",
    "            # # Conv1d Layer 2: Input Channels = hidden_features, Output Channels = hidden_features\n",
    "            # ('conv2', nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=self.stride)),\n",
    "            # ('relu2', nn.ReLU()),\n",
    "            # ('norm2', nn.BatchNorm1d(self.hidden_dim)),\n",
    "            # ('pool2', nn.MaxPool1d(kernel_size=kernel_size, stride=1, padding=pool_padding)),\n",
    "\n",
    "            # # Conv1d Layer 2: Input Channels = hidden_features, Output Channels = hidden_features\n",
    "            # ('conv3', nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=self.stride)),\n",
    "            # ('relu3', nn.ReLU()),\n",
    "            # ('norm3', nn.BatchNorm1d(self.hidden_dim)),\n",
    "            # ('pool3', nn.MaxPool1d(kernel_size=kernel_size, stride=1, padding=pool_padding)),\n",
    "        ]))\n",
    "\n",
    "        # Classifier - might need to add more units\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.hidden_dim * 179, 1),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear layer followed by Relu activation function\n",
    "        # The Sigmoid activation function outputs the value between 0 and 1\n",
    "        # 16 ==> 188\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            # # Conv1d Layer 1: Input Channels = hidden_features, Output Channels = hidden_features\n",
    "            # ('conv1', nn.ConvTranspose1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=self.stride)),\n",
    "            # ('relu1', nn.ReLU()),\n",
    "            # ('norm1', nn.BatchNorm1d(self.hidden_dim)),\n",
    "\n",
    "            # # Conv1d Layer 1: Input Channels = hidden_features, Output Channels = hidden_features\n",
    "            # ('conv2', nn.ConvTranspose1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=self.stride)),\n",
    "            # ('relu2', nn.ReLU()),\n",
    "            # ('norm2', nn.BatchNorm1d(self.hidden_dim)),\n",
    "\n",
    "            # Conv1d Layer 1: Input Channels = hidden_features, Output Channels = 1\n",
    "            ('conv3', nn.ConvTranspose1d(self.hidden_dim, 1, kernel_size=kernel_size, stride=self.stride)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('norm3', nn.BatchNorm1d(1)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # our input is (n_batch, length_sequence) we need to add a dimension for n_channels (n_batch, n_channels, length_sequence)\n",
    "        # for the Conv1D layers\n",
    "        encoded = self.encoder(x[:, None, :]) \n",
    "        # print(encoded.shape)\n",
    "        pred = self.classifier(encoded)\n",
    "        # print(pred.shape)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return pred, decoded.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527b4d1-95dd-42ec-92c7-5b2d6668995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ECGDataset(df.drop([\"target\", \"class\"]), df.select(\"target\"))\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# hyperparameters\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "epochs = 10\n",
    "input_dim = dataset[0][0].shape[0]\n",
    "hidden_dim = 128\n",
    "batch_size = 32\n",
    "kernel_size = 9\n",
    "stride = 1\n",
    "\n",
    "model = ConvAutoEncoder(input_dim, hidden_dim, kernel_size, stride)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "mse_loss_function = torch.nn.MSELoss()\n",
    "bce_loss_function = torch.nn.BCELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = 1e-1,\n",
    "    weight_decay = 1e-8\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer, \n",
    "#     step_size=4, \n",
    "#     gamma=0.1, \n",
    "# )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode=\"min\",\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=gen,\n",
    "    # num_workers=4,\n",
    "    # persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=gen,\n",
    "    # num_workers=4,\n",
    "    # persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b9510-6f4e-4c14-bbcc-a4bb3d397ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = BinaryAccuracy()\n",
    "confusion_matrix = BinaryConfusionMatrix()\n",
    "f1_score = BinaryF1Score()\n",
    "precision = BinaryPrecision()\n",
    "recall = BinaryRecall()\n",
    "\n",
    "accuracy_history = []\n",
    "confusion_matrix_history = []\n",
    "f1_score_history = []\n",
    "precision_history = []\n",
    "recall_history = []\n",
    "\n",
    "outputs = []\n",
    "train_batch_losses = []\n",
    "train_epoch_losses = []\n",
    "val_batch_losses = []\n",
    "val_epoch_losses = []\n",
    "\n",
    "train_batch_mse_losses = []\n",
    "train_epoch_mse_losses = []\n",
    "train_batch_bce_losses = []\n",
    "train_epoch_bce_losses = []\n",
    "\n",
    "val_batch_mse_losses = []\n",
    "val_epoch_mse_losses = []\n",
    "val_batch_bce_losses = []\n",
    "val_epoch_bce_losses = []\n",
    "\n",
    "alpha_mse = 1.0\n",
    "alpha_bce = 1.0\n",
    "\n",
    "for epoch in (pbar := tqdm(range(epochs), desc=f\"lr={scheduler.get_last_lr()[0]:0.6f}\")):\n",
    "    ####################\n",
    "    # Train\n",
    "    ####################\n",
    "    model.train()\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_mse_loss = 0\n",
    "    train_epoch_bce_loss = 0\n",
    "    val_epoch_mse_loss = 0\n",
    "    val_epoch_bce_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        X, target = batch\n",
    "        \n",
    "        # Output of Autoencoder\n",
    "        pred, reconstructed = model(X)\n",
    "        \n",
    "        # Calculating the loss function\n",
    "        mse_loss = mse_loss_function(reconstructed, X)\n",
    "        bce_loss = bce_loss_function(pred, target)\n",
    "        loss = alpha_mse * mse_loss + alpha_bce * bce_loss\n",
    "        \n",
    "        optimizer.zero_grad() # The gradients are set to zero,\n",
    "        loss.backward() # the gradients are computed and stored.\n",
    "        optimizer.step() # .step() performs parameter update\n",
    "        \n",
    "        # Storing the losses in a list for plotting\n",
    "        train_batch_losses.append(loss.item())\n",
    "        train_epoch_loss += loss.item()\n",
    "        train_batch_mse_losses.append(mse_loss.item())\n",
    "        train_epoch_mse_loss += mse_loss.item()\n",
    "        train_batch_bce_losses.append(bce_loss.item())\n",
    "        train_epoch_bce_loss += bce_loss.item()\n",
    "\n",
    "    # TODO: averaging this is wrong, should sum them average\n",
    "    train_epoch_mean_loss = epoch_loss / i\n",
    "    train_epoch_losses.append(train_epoch_mean_loss)\n",
    "    train_mean_epoch_mse_loss = train_epoch_mse_loss / i\n",
    "    train_epoch_mse_losses.append(train_mean_epoch_mse_loss)\n",
    "    train_mean_epoch_bce_loss = train_epoch_bce_loss / i\n",
    "    train_epoch_bce_losses.append(train_mean_epoch_bce_loss)\n",
    "    \n",
    "    ####################\n",
    "    # Validation\n",
    "    ####################\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        val_epoch_loss = 0\n",
    "        for j, batch in enumerate(train_dataloader):\n",
    "            X, target = batch\n",
    "            pred, reconstructed = model(X)            \n",
    "            mse_loss = mse_loss_function(reconstructed, X)\n",
    "            bce_loss = bce_loss_function(pred, target)\n",
    "\n",
    "            val_epoch_loss = alpha_mse * mse_loss + alpha_bce * bce_loss\n",
    "            val_batch_losses.append(val_epoch_loss.item())\n",
    "            val_epoch_loss += val_epoch_loss.item()\n",
    "            \n",
    "            val_batch_mse_losses.append(mse_loss.item())\n",
    "            val_epoch_mse_loss += mse_loss.item()\n",
    "            val_batch_bce_losses.append(bce_loss.item())\n",
    "            val_epoch_bce_loss += bce_loss.item()\n",
    "\n",
    "            accuracy.update(pred.squeeze(), target.squeeze().int())\n",
    "            confusion_matrix.update(pred.squeeze(), target.squeeze().int())\n",
    "            f1_score.update(pred.squeeze(), target.squeeze().int())\n",
    "            precision.update(pred.squeeze(), target.squeeze().int())\n",
    "            recall.update(pred.squeeze(), target.squeeze().int())\n",
    "\n",
    "        # TODO: averaging this is wrong, should sum them average\n",
    "        val_mean_epoch_loss = val_epoch_loss / j\n",
    "        val_mean_epoch_mse_loss = val_epoch_mse_loss / j\n",
    "        val_mean_epoch_bce_loss = val_epoch_bce_loss / j\n",
    "        val_epoch_losses.append(val_mean_epoch_loss)\n",
    "        val_epoch_mse_losses.append(val_mean_epoch_mse_loss)\n",
    "        val_epoch_bce_losses.append(val_mean_epoch_bce_loss)\n",
    "\n",
    "        accuracy_history.append(accuracy.compute())\n",
    "        confusion_matrix_history.append(accuracy.compute())\n",
    "        f1_score_history.append(accuracy.compute())\n",
    "        precision_history.append(accuracy.compute())\n",
    "        recall_history.append(accuracy.compute())\n",
    "\n",
    "    # scheduler.step()\n",
    "    val_epoch_loss = alpha_mse * val_mean_epoch_mse_loss + alpha_bce * val_mean_epoch_bce_loss\n",
    "    scheduler.step(val_epoch_loss) # update the learning rate if not learning\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    pbar.set_description(f\"lr={lr:0.6f}\")\n",
    "    print(f\"{epoch} (lr={lr:0.6f}) - train loss: ({train_mean_epoch_mse_loss:0.6f}, {train_mean_epoch_bce_loss:0.6f}) | val loss: ({val_mean_epoch_mse_loss:0.6f}, {val_mean_epoch_bce_loss:0.6f})\")\n",
    "    \n",
    "    # outputs.append((epochs, batch, reconstructed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674a3a9-59fc-4bf6-accb-b5f26edda69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "batch_losses_df = pl.DataFrame({\n",
    "    \"batch\": [x for x in range(0, len(train_batch_losses))] + [x for x in range(0, len(val_batch_losses))],\n",
    "    \"loss\": train_batch_losses + val_batch_losses,\n",
    "    \"mode\": [\"train\"] * len(train_batch_losses) + [\"validation\"] * len(val_batch_losses)\n",
    "})\n",
    "\n",
    "epoch_losses_df = pl.DataFrame({\n",
    "    \"epoch\": [x for x in range(0, len(train_epoch_losses))] + [x for x in range(0, len(val_epoch_losses))],\n",
    "    \"loss\": train_epoch_losses + val_epoch_losses,\n",
    "    \"mode\": [\"train\"] * len(train_epoch_losses) + [\"validation\"] * len(val_epoch_losses)\n",
    "})\n",
    "\n",
    "batch_loss_chart = (\n",
    "    alt.Chart(batch_losses_df.to_pandas(), title=\"Loss per Batch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='batch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "epoch_loss_chart = (\n",
    "    alt.Chart(epoch_losses_df.to_pandas(), title=\"Average Loss per Epoch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='epoch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "# MSE Loss\n",
    "batch_mse_losses_df = pl.DataFrame({\n",
    "    \"batch\": [x for x in range(0, len(train_batch_mse_losses))] + [x for x in range(0, len(val_batch_mse_losses))],\n",
    "    \"loss\": train_batch_mse_losses + val_batch_mse_losses,\n",
    "    \"mode\": [\"train\"] * len(train_batch_mse_losses) + [\"validation\"] * len(val_batch_mse_losses)\n",
    "})\n",
    "\n",
    "epoch_mse_losses_df = pl.DataFrame({\n",
    "    \"epoch\": [x for x in range(0, len(train_epoch_mse_losses))] + [x for x in range(0, len(val_epoch_mse_losses))],\n",
    "    \"loss\": train_epoch_mse_losses + val_epoch_mse_losses,\n",
    "    \"mode\": [\"train\"] * len(train_epoch_mse_losses) + [\"validation\"] * len(val_epoch_mse_losses)\n",
    "})\n",
    "\n",
    "batch_mse_loss_chart = (\n",
    "    alt.Chart(batch_mse_losses_df.to_pandas(), title=\"MSE Loss per Batch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='batch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "epoch_mse_loss_chart = (\n",
    "    alt.Chart(epoch_mse_losses_df.to_pandas(), title=\"Average MSE Loss per Epoch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='epoch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "\n",
    "# BCE Loss\n",
    "batch_bce_losses_df = pl.DataFrame({\n",
    "    \"batch\": [x for x in range(0, len(train_batch_bce_losses))] + [x for x in range(0, len(val_batch_bce_losses))],\n",
    "    \"loss\": train_batch_bce_losses + val_batch_bce_losses,\n",
    "    \"mode\": [\"train\"] * len(train_batch_bce_losses) + [\"validation\"] * len(val_batch_bce_losses)\n",
    "})\n",
    "\n",
    "\n",
    "epoch_bce_losses_df = pl.DataFrame({\n",
    "    \"epoch\": [x for x in range(0, len(train_epoch_bce_losses))] + [x for x in range(0, len(val_epoch_bce_losses))],\n",
    "    \"loss\": train_epoch_bce_losses + val_epoch_bce_losses,\n",
    "    \"mode\": [\"train\"] * len(train_epoch_bce_losses) + [\"validation\"] * len(val_epoch_bce_losses)\n",
    "})\n",
    "\n",
    "batch_bce_loss_chart = (\n",
    "    alt.Chart(batch_bce_losses_df.to_pandas(), title=\"BCE Loss per Batch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='batch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "epoch_bce_loss_chart = (\n",
    "    alt.Chart(epoch_bce_losses_df.to_pandas(), title=\"Average BCE Loss per Epoch\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x='epoch:Q',\n",
    "        y='loss:Q',\n",
    "        color='mode:N',\n",
    "    ).properties(height=150, width=400)\n",
    ")\n",
    "\n",
    "(batch_loss_chart | epoch_loss_chart) & (batch_mse_loss_chart | epoch_mse_loss_chart) & (batch_bce_loss_chart | epoch_bce_loss_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761ede2-a2fd-47ce-927c-f30be8b9515c",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef7958-e24e-40dc-ae06-f0b0eda842d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to(torch.int).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ed582-00eb-4dd7-b247-b788b0ce7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [nBatch, nChannels, length]\n",
    "hidden_features = 128\n",
    "# stride need to be 1 and kernel_size uneven for the padding to work and dimensions align\n",
    "kernel_size = 9\n",
    "\n",
    "# encode\n",
    "print(\"Encoding:\")\n",
    "m = nn.Conv1d(1, hidden_features, kernel_size=kernel_size, stride=1)\n",
    "input = torch.randn(32, 1, 187)\n",
    "output = m(input)\n",
    "print(\"Conv:     \", input.shape, output.shape)\n",
    "\n",
    "norm_input = output\n",
    "norm = nn.BatchNorm1d(hidden_features)\n",
    "norm_output = norm(norm_input)\n",
    "print(\"Norm:     \", norm_input.shape, norm_output.shape)\n",
    "\n",
    "pool_input = norm_output\n",
    "pool = nn.MaxPool1d(kernel_size=kernel_size, stride=1, padding=(kernel_size-1)//2) # (kernel_size-1)//2 - https://stackoverflow.com/a/71022586\n",
    "pool_output = pool(pool_input)\n",
    "print(\"Pool:     \", pool_input.shape, pool_output.shape)\n",
    "\n",
    "\n",
    "# decode\n",
    "print(\"\\nDecoding:\")\n",
    "conv_input = pool_output\n",
    "conv_input = norm_output\n",
    "conv = nn.ConvTranspose1d(hidden_features, 1, kernel_size=kernel_size, stride=1)\n",
    "conv_output = conv(conv_input)\n",
    "print(\"ConvTrans:\", conv_input.shape, conv_output.shape)\n",
    "\n",
    "norm_input = conv_output\n",
    "norm = nn.BatchNorm1d(1)\n",
    "norm_output = norm(norm_input)\n",
    "print(\"Norm:     \", norm_input.shape, norm_output.shape)\n",
    "\n",
    "\n",
    "# linear_input = norm_output\n",
    "# linear = nn.Linear(32, hidden_features, 187)\n",
    "# linear_output = linear(linear_input)\n",
    "# print(\"Linear:   \", linear_input.shape, linear_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733164d-3f33-48da-8686-8be27d9ae26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4, 5, 6)\n",
    "a = a[:, :, None, :]\n",
    "a.shape, a.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f58f2f-33d1-4bbc-81bf-3f208a9158dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fddee1-0de2-4254-8955-2f063277b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(32, 187).view(32, 1, 187).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef054899-e1dc-433c-9108-084cdc27995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(kernel_size-1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefac9a-4386-482e-9b8d-3110a68acfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ECGDataset(df.drop([\"target\", \"class\"]), df.select(\"target\"))\n",
    "dataset.get_row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb160fa8-7dd1-4062-ab8a-51a59d3322c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a8400-6281-49a2-89bf-3097ad6d6918",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(5, 1)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de72ee-6ab5-48e7-bcbc-99de08a86544",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013e84f-21a2-43e1-afb3-d91ea8f9d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f520d4-b1e3-4bf4-9e06-afec83a242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Threshold(0.5, 0.0)\n",
    "input = torch.randn(2)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63883a68-10db-4181-9a02-f6af14fbc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ee8d7-f647-4394-a300-560986deec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "target = torch.rand(3, 2, requires_grad=False)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8a321-7d7e-4747-a3a2-2991da683aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(input), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03f9f6-2845-4a3c-b748-6d480e4a705a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
